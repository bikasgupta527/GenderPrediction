{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I19BvSXcavQL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense,Conv2D,MaxPooling2D,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "pqO4UiO2b3RJ",
    "outputId": "77fb7c0c-244a-485e-d2c0-78932afb9ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n",
      "Found 60 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "gen=ImageDataGenerator(rescale=1/255.0,rotation_range=90,vertical_flip=True,horizontal_flip=True)\n",
    "train_itr=gen.flow_from_directory(\"./gender_training/gender_training/train\",target_size=(90,90),color_mode='grayscale',batch_size=32,class_mode='binary')\n",
    "val_itr=gen.flow_from_directory(\"./gender_training/gender_training/val\",target_size=(90,90),color_mode='grayscale',batch_size=32,class_mode='binary')\n",
    "test_itr=gen.flow_from_directory(\"./gender_training/gender_training/test\",target_size=(90,90),color_mode='grayscale',batch_size=32,class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "a5JKzYjMvrbs",
    "outputId": "b46c2cf3-9b47-4497-a971-6dabcec2721f"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(90,90,1)))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.4115 - accuracy: 0.8167 - val_loss: 0.6182 - val_accuracy: 0.7500\n",
      "Epoch 2/40\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.3301 - accuracy: 0.8333 - val_loss: 0.5250 - val_accuracy: 0.7250\n",
      "Epoch 3/40\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.3749 - accuracy: 0.8167 - val_loss: 0.5280 - val_accuracy: 0.7750\n",
      "Epoch 4/40\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.3980 - accuracy: 0.8167 - val_loss: 0.5257 - val_accuracy: 0.7000\n",
      "Epoch 5/40\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.3127 - accuracy: 0.8833 - val_loss: 0.5476 - val_accuracy: 0.7250\n",
      "Epoch 6/40\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.3297 - accuracy: 0.8333 - val_loss: 0.5602 - val_accuracy: 0.7750\n",
      "Epoch 7/40\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.2857 - accuracy: 0.8833 - val_loss: 0.4663 - val_accuracy: 0.8250\n",
      "Epoch 8/40\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.3021 - accuracy: 0.8333 - val_loss: 0.5180 - val_accuracy: 0.8000\n",
      "Epoch 9/40\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.3628 - accuracy: 0.8167 - val_loss: 0.5460 - val_accuracy: 0.7750\n",
      "Epoch 10/40\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2782 - accuracy: 0.9000 - val_loss: 0.4487 - val_accuracy: 0.7750\n",
      "Epoch 11/40\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2924 - accuracy: 0.8833 - val_loss: 0.6641 - val_accuracy: 0.7750\n",
      "Epoch 12/40\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.3823 - accuracy: 0.8167 - val_loss: 0.5663 - val_accuracy: 0.7500\n",
      "Epoch 13/40\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.4039 - accuracy: 0.8167 - val_loss: 0.5456 - val_accuracy: 0.6750\n",
      "Epoch 14/40\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.3058 - accuracy: 0.8333 - val_loss: 0.7161 - val_accuracy: 0.7750\n",
      "Epoch 15/40\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.3140 - accuracy: 0.8333 - val_loss: 0.6449 - val_accuracy: 0.7000\n",
      "Epoch 16/40\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.3274 - accuracy: 0.8000 - val_loss: 0.5403 - val_accuracy: 0.7250\n",
      "Epoch 17/40\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.3201 - accuracy: 0.8833 - val_loss: 0.5538 - val_accuracy: 0.7250\n",
      "Epoch 18/40\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2858 - accuracy: 0.8833 - val_loss: 0.5774 - val_accuracy: 0.7500\n",
      "Epoch 19/40\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.3456 - accuracy: 0.8500 - val_loss: 0.5091 - val_accuracy: 0.7750\n",
      "Epoch 20/40\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.3108 - accuracy: 0.8667 - val_loss: 0.5206 - val_accuracy: 0.7750\n",
      "Epoch 21/40\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.5295 - accuracy: 0.8000 - val_loss: 0.5003 - val_accuracy: 0.8000\n",
      "Epoch 22/40\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.2864 - accuracy: 0.8833 - val_loss: 0.5365 - val_accuracy: 0.6750\n",
      "Epoch 23/40\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.3844 - accuracy: 0.8167 - val_loss: 0.5300 - val_accuracy: 0.6750\n",
      "Epoch 24/40\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.3208 - accuracy: 0.8500 - val_loss: 0.6058 - val_accuracy: 0.7500\n",
      "Epoch 25/40\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.3954 - accuracy: 0.8333 - val_loss: 0.6364 - val_accuracy: 0.7500\n",
      "Epoch 26/40\n",
      "2/2 [==============================] - 0s 174ms/step - loss: 0.3256 - accuracy: 0.8667 - val_loss: 0.5443 - val_accuracy: 0.7000\n",
      "Epoch 27/40\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.3191 - accuracy: 0.8500 - val_loss: 0.5395 - val_accuracy: 0.7250\n",
      "Epoch 28/40\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.3036 - accuracy: 0.8333 - val_loss: 0.7162 - val_accuracy: 0.7500\n",
      "Epoch 29/40\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.3400 - accuracy: 0.8000 - val_loss: 0.5405 - val_accuracy: 0.8000\n",
      "Epoch 30/40\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.2739 - accuracy: 0.8833 - val_loss: 0.5654 - val_accuracy: 0.7250\n",
      "Epoch 31/40\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.3324 - accuracy: 0.8167 - val_loss: 0.5849 - val_accuracy: 0.7750\n",
      "Epoch 32/40\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.3471 - accuracy: 0.8167 - val_loss: 0.6249 - val_accuracy: 0.7500\n",
      "Epoch 33/40\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.3820 - accuracy: 0.8167 - val_loss: 0.5823 - val_accuracy: 0.7000\n",
      "Epoch 34/40\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.3193 - accuracy: 0.8167 - val_loss: 0.4741 - val_accuracy: 0.7500\n",
      "Epoch 35/40\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.2560 - accuracy: 0.9167 - val_loss: 0.6734 - val_accuracy: 0.7750\n",
      "Epoch 36/40\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.3456 - accuracy: 0.8833 - val_loss: 0.5956 - val_accuracy: 0.7750\n",
      "Epoch 37/40\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.2734 - accuracy: 0.9000 - val_loss: 0.5561 - val_accuracy: 0.7500\n",
      "Epoch 38/40\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.2775 - accuracy: 0.8333 - val_loss: 0.5934 - val_accuracy: 0.7250\n",
      "Epoch 39/40\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.3072 - accuracy: 0.8667 - val_loss: 0.7340 - val_accuracy: 0.7500\n",
      "Epoch 40/40\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.2985 - accuracy: 0.8667 - val_loss: 0.6331 - val_accuracy: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2284ff5cd60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(test_itr,epochs=40,validation_data=val_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "X7gqHDydwLZo",
    "outputId": "154d2a5c-cb96-40a8-8eef-f616f007bb2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/10 [=====>........................] - ETA: 0s - loss: 0.2901 - accuracy: 0.8500WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n",
      " 2/10 [=====>........................] - 0s 40ms/step - loss: 0.2901 - accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2900760769844055, 0.8500000238418579]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_itr,steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_FfqzQ5sBBDg",
    "outputId": "be2dd487-f135-4690-e1d2-ef64a9d31aec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'female': 0, 'male': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_itr.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SreKAiJ-geiQ",
    "outputId": "7ec3df56-ae81-4207-e77e-95945bebfd78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread(\"../vikas2.JPEG\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#gray=gray/255.0\n",
    "faceDetect=cv2.CascadeClassifier('f:haarcascade/haarcascade_frontalface_default.xml')\n",
    "faces = faceDetect.detectMultiScale(gray)\n",
    "for (x,y,w,h) in faces:\n",
    "    face=gray[y:y+h,x:x+w]\n",
    "    face=cv2.resize(face,(90,90))\n",
    "    print(np.argmax(model.predict(face.reshape(1,90,90,1)),axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpDROMGcEIcS"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_cascade  = cv2.CascadeClassifier('f:haarcascade/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    asdf,img = cap.read()\n",
    "    grayimg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(grayimg,1.1,4)\n",
    "    \n",
    "    for x,y,w,h in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),3)\n",
    "        face=grayimg[y:y+h,x:x+w]\n",
    "        face=cv2.resize(face,(90,90))\n",
    "    x = np.argmax(model.predict(face.reshape(1,90,90,1)),axis=-1)\n",
    "    if x == 1:\n",
    "        cv2.putText(img,'Male',(50,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
    "    elif x==0:\n",
    "        cv2.putText(img,'Female',(50,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
    "        \n",
    "    cv2.imshow('Live Video',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k==27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "9_Aug_Gender_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
